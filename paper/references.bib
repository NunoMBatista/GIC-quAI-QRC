@article{belisGuidedQuantumCompression2024,
  title = {Guided {{Quantum Compression}} for {{High Dimensional Data Classification}}},
  author = {Belis, Vasilis and Odagiu, Patrick and Grossi, Michele and Reiter, Florentin and Dissertori, G{\"u}nther and Vallecorsa, Sofia},
  year = {2024},
  month = sep,
  journal = {Machine Learning: Science and Technology},
  volume = {5},
  number = {3},
  eprint = {2402.09524},
  primaryclass = {quant-ph},
  pages = {035010},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ad5fdd},
  urldate = {2025-05-23},
  abstract = {Quantum machine learning provides a fundamentally different approach to analyzing data. However, many interesting datasets are too complex for currently available quantum computers. Present quantum machine learning applications usually diminish this complexity by reducing the dimensionality of the data, e.g., via auto-encoders, before passing it through the quantum models. Here, we design a classical-quantum paradigm that unifies the dimensionality reduction task with a quantum classification model into a single architecture: the guided quantum compression model. We exemplify how this architecture outperforms conventional quantum machine learning approaches on a challenging binary classification problem: identifying the Higgs boson in proton-proton collisions at the LHC. Furthermore, the guided quantum compression model shows better performance compared to the deep learning benchmark when using solely the kinematic variables in our dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Experiment,Quantum Physics},
  file = {/home/nuno/Zotero/storage/KTQ3F2MY/Belis et al. - 2024 - Guided Quantum Compression for High Dimensional Data Classification.pdf}
}

@article{bengioLearningDeepArchitecturesa,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Yoshua},
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/QS4TT277/Bengio - Learning Deep Architectures for AI.pdf}
}

@misc{domingoOptimalQuantumReservoir2022,
  title = {Optimal Quantum Reservoir Computing for the {{NISQ}} Era},
  author = {Domingo, L. and Carlo, G. and Borondo, F.},
  year = {2022},
  month = may,
  number = {arXiv:2205.10107},
  eprint = {2205.10107},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10107},
  urldate = {2025-05-26},
  abstract = {Universal fault-tolerant quantum computers require millions of qubits with low error rates. Since this technology is years ahead, noisy intermediate-scale quantum (NISQ) computation is receiving tremendous interest. In this setup, quantum reservoir computing is a relevant machine learning algorithm. Its simplicity of training and implementation allows to perform challenging computations on today available machines. In this Letter, we provide a criterion to select optimal quantum reservoirs, requiring few and simple gates. Our findings demonstrate that they render better results than other commonly used models with significantly less gates, and also provide insight on the theoretical gap between quantum reservoir computing and the theory of quantum states complexity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantum Physics},
  file = {/home/nuno/Zotero/storage/LGLWR7RK/Domingo et al. - 2022 - Optimal quantum reservoir computing for the NISQ era.pdf}
}

@article{estevaGuideDeepLearning2019a,
  title = {A Guide to Deep Learning in Healthcare},
  author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {24--29},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-018-0316-z},
  urldate = {2025-05-26},
  langid = {english},
  file = {/home/nuno/Zotero/storage/RC54H4LK/Esteva et al. - 2019 - A guide to deep learning in healthcare.pdf}
}

@article{fujiiHarnessingDisorderedQuantum2017,
  title = {Harnessing Disordered Quantum Dynamics for Machine Learning},
  author = {Fujii, Keisuke and Nakajima, Kohei},
  year = {2017},
  month = aug,
  journal = {Physical Review Applied},
  volume = {8},
  number = {2},
  eprint = {1602.08159},
  primaryclass = {quant-ph},
  pages = {024030},
  issn = {2331-7019},
  doi = {10.1103/PhysRevApplied.8.024030},
  urldate = {2025-05-26},
  abstract = {Quantum computer has an amazing potential of fast information processing. However, realisation of a digital quantum computer is still a challenging problem requiring highly accurate controls and key application strategies. Here we propose a novel platform, quantum reservoir computing, to solve these issues successfully by exploiting natural quantum dynamics, which is ubiquitous in laboratories nowadays, for machine learning. In this framework, nonlinear dynamics including classical chaos can be universally emulated in quantum systems. A number of numerical experiments show that quantum systems consisting of at most seven qubits possess computational capabilities comparable to conventional recurrent neural networks of 500 nodes. This discovery opens up a new paradigm for information processing with artificial intelligence powered by quantum physics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Chaotic Dynamics,Quantum Physics},
  file = {/home/nuno/Zotero/storage/832RUGD2/Fujii and Nakajima - 2017 - Harnessing disordered quantum dynamics for machine learning.pdf}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  year = {2006},
  month = jul,
  journal = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  urldate = {2025-05-26},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ``autoencoder'' networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/C69T23J9/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf}
}

@misc{HttpsArxivorgPdfa,
  title = {{{https://arxiv.org/pdf/2306.11727}}},
  urldate = {2025-05-26},
  howpublished = {https://arxiv.org/pdf/2306.11727},
  file = {/home/nuno/Zotero/storage/EUFJ7A5Q/2306.pdf}
}

@article{jolliffePrincipalComponentAnalysis2016,
  title = {Principal Component Analysis: A Review and Recent Developments},
  shorttitle = {Principal Component Analysis},
  author = {Jolliffe, Ian T. and Cadima, Jorge},
  year = {2016},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {374},
  number = {2065},
  pages = {20150202},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2015.0202},
  urldate = {2025-05-26},
  abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not               a priori               , hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/FJCHXF7X/Jolliffe and Cadima - 2016 - Principal component analysis a review and recent developments.pdf}
}

@inproceedings{kornjavcaLargescaleQuantumReservoir2024,
  title = {Large-Scale Quantum Reservoir Learning with an Analog Quantum Computer},
  author = {Kornjavca, Milan and Hu, Hong-Ye and Zhao, Chen and Wurtz, Jonathan and Weinberg, Phillip and Hamdan, Majd and Zhdanov, Andrii and Cantu, Sergio and Zhou, Hengyun and Bravo, Rodrigo Araiza and Bagnall, Kevin and Basham, J. and Campo, Joseph and Choukri, Adam and DeAngelo, Robert and Frederick, Paige and Haines, David and Hammett, Julian and Hsu, Ning and Hu, Ming-Guang and Huber, Florian and Jepsen, P. N. and Jia, Ningyuan and Karolyshyn, Thomas and Kwon, Minho and Long, John and Lopatin, Jonathan and Lukin, Alexander and Macri, Tommaso and Markovi'c, Ognjen and {Mart'inez-Mart'inez}, Luis A. and Meng, Xianmei and Ostroumov, E. and Paquette, David and Robinson, John M. and Rodriguez, Pedro Sales and Singh, Anshuman and Sinha, Nandan and Thoreen, Henry and Wan, Noel and {Waxman-Lenz}, Daniel and Wong, Tak and Wu, Kai-Hsin and Lopes, Pedro L. S. and Boger, Y. and Gemelke, N. and Kitagawa, Takuya and Keesling, A. and Gao, Xun and Bylinskii, A. and Yelin, S. and Liu, Fangli and Wang, Sheng-Tao},
  year = {2024},
  month = jul,
  urldate = {2025-05-20},
  abstract = {Quantum machine learning has gained considerable attention as quantum technology advances, presenting a promising approach for efficiently learning complex data patterns. Despite this promise, most contemporary quantum methods require significant resources for variational parameter optimization and face issues with vanishing gradients, leading to experiments that are either limited in scale or lack potential for quantum advantage. To address this, we develop a general-purpose, gradient-free, and scalable quantum reservoir learning algorithm that harnesses the quantum dynamics of neutral-atom analog quantum computers to process data. We experimentally implement the algorithm, achieving competitive performance across various categories of machine learning tasks, including binary and multi-class classification, as well as timeseries prediction. Effective and improving learning is observed with increasing system sizes of up to 108 qubits, demonstrating the largest quantum machine learning experiment to date. We further observe comparative quantum kernel advantage in learning tasks by constructing synthetic datasets based on the geometric differences between generated quantum and classical data kernels. Our findings demonstrate the potential of utilizing classically intractable quantum correlations for effective machine learning. We expect these results to stimulate further extensions to different quantum hardware and machine learning paradigms, including early fault-tolerant hardware and generative machine learning tasks.},
  file = {/home/nuno/Zotero/storage/899TPMHP/Kornjavca et al. - 2024 - Large-scale quantum reservoir learning with an analog quantum computer.pdf}
}

@article{mccleanBarrenPlateausQuantum2018,
  title = {Barren Plateaus in Quantum Neural Network Training Landscapes},
  author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
  year = {2018},
  month = nov,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  eprint = {1803.11173},
  primaryclass = {quant-ph},
  pages = {4812},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07090-4},
  urldate = {2025-05-26},
  abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Quantum Physics},
  file = {/home/nuno/Zotero/storage/6KHLIB5R/McClean et al. - 2018 - Barren plateaus in quantum neural network training landscapes.pdf}
}

@misc{shlensTutorialPrincipalComponent2014,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  year = {2014},
  month = apr,
  number = {arXiv:1404.1100},
  eprint = {1404.1100},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1404.1100},
  urldate = {2025-05-26},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/nuno/Zotero/storage/GKNSGH3Z/Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf}
}

@article{tanakaRecentAdvancesPhysical2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.03.005},
  urldate = {2025-05-26},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/ZFKF5FDQ/Tanaka et al. - 2019 - Recent advances in physical reservoir computing A review.pdf}
}

@misc{zamparoDeepAutoencodersDimensionality2015,
  title = {Deep {{Autoencoders}} for {{Dimensionality Reduction}} of {{High-Content Screening Data}}},
  author = {Zamparo, Lee and Zhang, Zhaolei},
  year = {2015},
  month = jan,
  number = {arXiv:1501.01348},
  eprint = {1501.01348},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1501.01348},
  urldate = {2025-05-26},
  abstract = {High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/nuno/Zotero/storage/R6EMDSDT/Zamparo and Zhang - 2015 - Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data.pdf}
}
