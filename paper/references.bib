@article{belisGuidedQuantumCompression2024,
  title = {Guided {{Quantum Compression}} for {{High Dimensional Data Classification}}},
  author = {Belis, Vasilis and Odagiu, Patrick and Grossi, Michele and Reiter, Florentin and Dissertori, G{\"u}nther and Vallecorsa, Sofia},
  year = {2024},
  month = sep,
  journal = {Machine Learning: Science and Technology},
  volume = {5},
  number = {3},
  eprint = {2402.09524},
  primaryclass = {quant-ph},
  pages = {035010},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ad5fdd},
  urldate = {2025-05-23},
  abstract = {Quantum machine learning provides a fundamentally different approach to analyzing data. However, many interesting datasets are too complex for currently available quantum computers. Present quantum machine learning applications usually diminish this complexity by reducing the dimensionality of the data, e.g., via auto-encoders, before passing it through the quantum models. Here, we design a classical-quantum paradigm that unifies the dimensionality reduction task with a quantum classification model into a single architecture: the guided quantum compression model. We exemplify how this architecture outperforms conventional quantum machine learning approaches on a challenging binary classification problem: identifying the Higgs boson in proton-proton collisions at the LHC. Furthermore, the guided quantum compression model shows better performance compared to the deep learning benchmark when using solely the kinematic variables in our dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Experiment,Quantum Physics},
  file = {/home/nuno/Zotero/storage/KTQ3F2MY/Belis et al. - 2024 - Guided Quantum Compression for High Dimensional Data Classification.pdf}
}

@article{bengioLearningDeepArchitecturesa,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Yoshua},
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/QS4TT277/Bengio - Learning Deep Architectures for AI.pdf}
}

@misc{domingoOptimalQuantumReservoir2022,
  title = {Optimal Quantum Reservoir Computing for the {{NISQ}} Era},
  author = {Domingo, L. and Carlo, G. and Borondo, F.},
  year = {2022},
  month = may,
  number = {arXiv:2205.10107},
  eprint = {2205.10107},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10107},
  urldate = {2025-05-26},
  abstract = {Universal fault-tolerant quantum computers require millions of qubits with low error rates. Since this technology is years ahead, noisy intermediate-scale quantum (NISQ) computation is receiving tremendous interest. In this setup, quantum reservoir computing is a relevant machine learning algorithm. Its simplicity of training and implementation allows to perform challenging computations on today available machines. In this Letter, we provide a criterion to select optimal quantum reservoirs, requiring few and simple gates. Our findings demonstrate that they render better results than other commonly used models with significantly less gates, and also provide insight on the theoretical gap between quantum reservoir computing and the theory of quantum states complexity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantum Physics},
  file = {/home/nuno/Zotero/storage/LGLWR7RK/Domingo et al. - 2022 - Optimal quantum reservoir computing for the NISQ era.pdf}
}

@article{estevaGuideDeepLearning2019a,
  title = {A Guide to Deep Learning in Healthcare},
  author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {24--29},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-018-0316-z},
  urldate = {2025-05-26},
  langid = {english},
  file = {/home/nuno/Zotero/storage/RC54H4LK/Esteva et al. - 2019 - A guide to deep learning in healthcare.pdf}
}

@article{fujiiHarnessingDisorderedQuantum2017,
  title = {Harnessing Disordered Quantum Dynamics for Machine Learning},
  author = {Fujii, Keisuke and Nakajima, Kohei},
  year = {2017},
  month = aug,
  journal = {Physical Review Applied},
  volume = {8},
  number = {2},
  eprint = {1602.08159},
  primaryclass = {quant-ph},
  pages = {024030},
  issn = {2331-7019},
  doi = {10.1103/PhysRevApplied.8.024030},
  urldate = {2025-05-26},
  abstract = {Quantum computer has an amazing potential of fast information processing. However, realisation of a digital quantum computer is still a challenging problem requiring highly accurate controls and key application strategies. Here we propose a novel platform, quantum reservoir computing, to solve these issues successfully by exploiting natural quantum dynamics, which is ubiquitous in laboratories nowadays, for machine learning. In this framework, nonlinear dynamics including classical chaos can be universally emulated in quantum systems. A number of numerical experiments show that quantum systems consisting of at most seven qubits possess computational capabilities comparable to conventional recurrent neural networks of 500 nodes. This discovery opens up a new paradigm for information processing with artificial intelligence powered by quantum physics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Chaotic Dynamics,Quantum Physics},
  file = {/home/nuno/Zotero/storage/832RUGD2/Fujii and Nakajima - 2017 - Harnessing disordered quantum dynamics for machine learning.pdf}
}

@inproceedings{kornjavcaLargescaleQuantumReservoir2024,
  title = {Large-Scale Quantum Reservoir Learning with an Analog Quantum Computer},
  author = {Kornjavca, Milan and Hu, Hong-Ye and Zhao, Chen and Wurtz, Jonathan and Weinberg, Phillip and Hamdan, Majd and Zhdanov, Andrii and Cantu, Sergio and Zhou, Hengyun and Bravo, Rodrigo Araiza and Bagnall, Kevin and Basham, J. and Campo, Joseph and Choukri, Adam and DeAngelo, Robert and Frederick, Paige and Haines, David and Hammett, Julian and Hsu, Ning and Hu, Ming-Guang and Huber, Florian and Jepsen, P. N. and Jia, Ningyuan and Karolyshyn, Thomas and Kwon, Minho and Long, John and Lopatin, Jonathan and Lukin, Alexander and Macri, Tommaso and Markovi'c, Ognjen and {Mart'inez-Mart'inez}, Luis A. and Meng, Xianmei and Ostroumov, E. and Paquette, David and Robinson, John M. and Rodriguez, Pedro Sales and Singh, Anshuman and Sinha, Nandan and Thoreen, Henry and Wan, Noel and {Waxman-Lenz}, Daniel and Wong, Tak and Wu, Kai-Hsin and Lopes, Pedro L. S. and Boger, Y. and Gemelke, N. and Kitagawa, Takuya and Keesling, A. and Gao, Xun and Bylinskii, A. and Yelin, S. and Liu, Fangli and Wang, Sheng-Tao},
  year = {2024},
  month = jul,
  urldate = {2025-05-20},
  abstract = {Quantum machine learning has gained considerable attention as quantum technology advances, presenting a promising approach for efficiently learning complex data patterns. Despite this promise, most contemporary quantum methods require significant resources for variational parameter optimization and face issues with vanishing gradients, leading to experiments that are either limited in scale or lack potential for quantum advantage. To address this, we develop a general-purpose, gradient-free, and scalable quantum reservoir learning algorithm that harnesses the quantum dynamics of neutral-atom analog quantum computers to process data. We experimentally implement the algorithm, achieving competitive performance across various categories of machine learning tasks, including binary and multi-class classification, as well as timeseries prediction. Effective and improving learning is observed with increasing system sizes of up to 108 qubits, demonstrating the largest quantum machine learning experiment to date. We further observe comparative quantum kernel advantage in learning tasks by constructing synthetic datasets based on the geometric differences between generated quantum and classical data kernels. Our findings demonstrate the potential of utilizing classically intractable quantum correlations for effective machine learning. We expect these results to stimulate further extensions to different quantum hardware and machine learning paradigms, including early fault-tolerant hardware and generative machine learning tasks.},
  file = {/home/nuno/Zotero/storage/899TPMHP/Kornjavca et al. - 2024 - Large-scale quantum reservoir learning with an analog quantum computer.pdf}
}

@article{mccleanBarrenPlateausQuantum2018,
  title = {Barren Plateaus in Quantum Neural Network Training Landscapes},
  author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
  year = {2018},
  month = nov,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  eprint = {1803.11173},
  primaryclass = {quant-ph},
  pages = {4812},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07090-4},
  urldate = {2025-05-26},
  abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Quantum Physics},
  file = {/home/nuno/Zotero/storage/6KHLIB5R/McClean et al. - 2018 - Barren plateaus in quantum neural network training landscapes.pdf}
}

@article{tanakaRecentAdvancesPhysical2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.03.005},
  urldate = {2025-05-26},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  langid = {english},
  file = {/home/nuno/Zotero/storage/ZFKF5FDQ/Tanaka et al. - 2019 - Recent advances in physical reservoir computing A review.pdf}
}
