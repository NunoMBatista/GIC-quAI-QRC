\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}


\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{float}
\usepackage{inputenc}
\usepackage{bm}
%\usepackage{amsmath}


% Add TikZ package and necessary libraries
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,backgrounds,shapes.geometric}

% Add circuitikz package for circuit diagrams
\usepackage{circuitikz}

% Add float control parameters to help with figure placement
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

% Force LaTeX to place figures earlier
\renewcommand{\dblfloatpagefraction}{0.7}
\renewcommand{\dbltopfraction}{0.8}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Medical Imaging Classification with Cold-Atom Reservoir Computing using Auto-Encoders and Surrogate-Driven Training\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
}

\author{\IEEEauthorblockN{Anonymous Authors}}
% \author{
% \IEEEauthorblockN{Nuno Batista}
% \IEEEauthorblockA{\textit{Department of Informatics Engineering} \\
% \textit{Faculty of Sciences and Technology, University of Coimbra}\\
% Coimbra, Portugal \\
% \href{mailto:nunomarquesbatista@gmail.com}{nunomarquesbatista@gmail.com}}

% % \and
% % \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% % \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% % \textit{name of organization (of Aff.)}\\
% % City, Country \\
% % email address or ORCID}
% }

\maketitle

%ADD SPACE TO BREATHE

\begin{abstract}
We introduce a quantum-classical hybrid
architecture for medical image classification based on neutral-atom quantum processors. This approach is designed to address the challenges of medical imaging, with a particular focus on tasks such as polyp detection and classification. By integrating an autoencoder guided by a quantum reservoir, the pipeline learns compact and discriminative representations of image data that are also well-suited for quantum reservoir computing. To overcome the non-differentiability of quantum measurements, we circumvent this `gradient barrier' by incorporating a differentiable surrogate model that simulates the behaviour of the quantum layer, enabling end-to-end backpropagation. The guided training process jointly optimizes for both image reconstruction and classification accuracy, ensuring that the latent representations are both meaningful and effective for quantum processing. In our implementation, image data is encoded as atom detuning parameters in a Rydberg Hamiltonian, and quantum embeddings are obtained through expectation values. These embeddings are then passed to a linear classifier, enabling faster training and inference compared to deep classical networks. Our experiments show that this method outperforms traditional approaches using PCA or unguided autoencoders. We also conduct ablation studies to evaluate the impact of quantum and training parameters, demonstrating the robustness and flexibility of the proposed pipeline for real-world medical imaging applications, even in the NISQ era.

\end{abstract}

\begin{IEEEkeywords}
Reservoir Computing, Quantum-Guided Autoencoding,
Neutral Atoms, Autoencoder, Dimensionality Reduction, 
Quantum Machine Learning, Hybrid Quantum-Classical Algorithms, 
Medical Image Classification, Quantum Surrogate Models
\end{IEEEkeywords}

%============================================
% hilbert
%============================================
\section{Introduction}

\subsection{Background and Motivation}
Advances in medical imaging have significantly improved 
disease diagnosis and treatment planning. For conditions 
like colorectal cancer, early detection of polyps through 
colonoscopy image analysis is critical for reducing mortality~\cite{estevaGuideDeepLearning2019}. 
Deep learning techniques, especially autoencoders, are widely 
used to extract compressed, informative features from 
high-dimensional images for classification and segmentation 
tasks~\cite{bengioLearningDeepArchitectures}. However, classical neural networks may struggle 
to capture intricate correlations in complex medical data.

Quantum computing offers novel opportunities 
for machine learning, particularly through quantum reservoir 
computing (QRC), where a physical quantum system processes 
classical inputs into high-dimensional nonlinear embeddings~\cite{tanakaRecentAdvancesPhysical2019,fujiiHarnessingDisorderedQuantum2017}. Recent works show that analog quantum systems, such as neutral-atom platforms, can serve as untrained reservoirs with rich dynamics for temporal and pattern recognition tasks~\cite{domingoOptimalQuantumReservoir2022,kornjavcaLargescaleQuantumReservoir2024}. 

In hybrid approaches, a classical encoder compresses image data, and a quantum reservoir expands the encoded features into a higher-dimensional Hilbert space, potentially boosting classification performance, as these expanded features might capture high-order correlations and nonlinearities that classical methods cannot tractably represent.

A major challenge in such hybrid quantum-classical models is the non-differentiability of quantum measurements, which obstructs gradient-based optimization. Additionally, tuning quantum parameters can suffer from barren plateaus, where gradients vanish in high-dimensional Hilbert spaces~\cite{mccleanBarrenPlateausQuantum2018}. To address this, we introduce a classical neural surrogate that emulates the quantum reservoir's input-output behavior. 
This surrogate enables end-to-end training via backpropagation, while the quantum system remains fixed and non-trainable and is only used to train the surrogate model and in downstream classifiers.

\subsection{Contributions of This Work}
We propose a quantum-guided autoencoder architecture that integrates a classical image encoder with a neutral-atom quantum reservoir.

A classical surrogate network of the reservoir itself enables 
gradient flow through the whole model during training.

Our results illustrate the viability of QRC for real-world medical 
tasks and offer a scalable path to hybrid quantum-classical learning,
even in the noisy intermediate-scale quantum (NISQ) era.



%============================================
% BACKGROUND 
%============================================
\section{Background and Related Work}

\subsection{Principles of Reservoir Computing}
Reservoir computing is a computational framework 
derived from recurrent neural networks (RNNs). It 
involves a fixed, high-dimensional dynamical system—the 
reservoir—that projects input data into a rich feature 
space. Only the output layer is trained, simplifying 
the learning process and reducing computational 
overhead. This approach is particularly effective 
for time-series prediction and pattern recognition tasks.

Mathematically, let \( u(t) \in \mathbb{R}^m \) be the input at time \( t \),
\( x(t) \in \mathbb{R}^n \) the reservoir state, and
\( y(t) \in \mathbb{R}^k \) the output. The reservoir dynamics
and output are given by:
\begin{equation}
    x(t) = f(W_{in} u(t) + W_{res} x(t-1))
\end{equation}
\begin{equation}
    y(t) = W_{out} x(t)
\end{equation}

%Where ff is a nonlinear activation function, WinWin​ and WW are fixed input and reservoir weight matrices, and WoutWout​ is the trained output weight matrix.

Where \( f \) is a nonlinear activation function,
\( W_{in} \) and \( W_{res} \) are fixed input and reservoir weight matrices, 
and \( W_{out} \) is the trained output weight matrix.

A diagram of a typical Reservoir Computing architecture is shown in `Fig.-\ref{fig:reservoir_architecture}'.

\input{images/diagrams/reservoir_architecture.tex}


\subsection{Quantum Reservoir Computing}

\subsubsection{Main Differences}
Quantum Reservoir Computing (QRC) extends the reservoir 
computing paradigm into the quantum domain. By leveraging quantum systems' 
inherent properties, such as superposition and entanglement, QRC aims 
to enhance computational capabilities. Implementations using quantum 
oscillators have shown promise in solving complex learning tasks, 
offering advantages over classical counterparts. Notably, large-scale 
experiments utilizing neutral-atom analog quantum computers have demonstrated 
the scalability and effectiveness of QRC in various machine learning applications~\cite{kornjavcaLargescaleQuantumReservoir2024}


In QRC, classical input data \( u(t) \) is encoded into quantum states \( |\psi(t)\rangle \),
which evolve under a fixed Hamiltonian \( H \):
\begin{equation}
    |\psi(t+1)\rangle = U |\psi(t)\rangle = e^{-iH\Delta t} |\psi(t)\rangle,
\end{equation}
where \( U \) is the unitary evolution operator. Measurements of observables \( \hat{O} \) yield outputs:
\begin{equation}
    y(t) = \langle \psi(t) | \hat{O} | \psi(t) \rangle.
\end{equation}
The output weights are trained classically, while the quantum reservoir remains fixed.

\subsubsection{Quantum Reservoir Computing with Neutral Atoms}
Neutral atom platforms, particularly those utilizing Rydberg states, have emerged as promising candidates for implementing QRC due to their scalability and controllable interactions.

In the work by M. Kornjača et al.~\cite{kornjavcaLargescaleQuantumReservoir2024}, a large-scale, gradient-free QRC algorithm was developed and experimentally implemented on a neutral-atom analog quantum computer. This system achieved competitive performance across various machine learning tasks, including classification and time-series prediction, demonstrating effective learning with increasing system sizes up to $108$ qubits.

The dynamics of such neutral atom systems can be described by the Rydberg Hamiltonian, which captures 
the essential physics of laser-driven interactions among atoms in Rydberg states. Following Kornjača et al.~\cite{kornjavcaLargescaleQuantumReservoir2024} the Hamiltonian for a system of neutral-atoms is given by:

\begin{equation}
    \begin{aligned}
        H(t) &= \dfrac{\Omega(t)}{2}\sum_j \left(\ket{g_j}\bra{r_j}+\ket{r_j}\bra{g_j}\right) \\
             &{{\quad}} + \sum_{j<k}V_{jk}n_jn_k - \sum_j \left[\Delta_{\mathrm{g}}(t) + \alpha_j\Delta_{\mathrm{l}}(t)\right] n_j,
    \end{aligned}
    \label{eq:rydberg_hamiltonian}
\end{equation}
where \( \Omega(t) \) is the global Rabi drive amplitude between a 
ground state \( |g_j\rangle \) and a highly-excited Rydberg state 
\( |r_j\rangle \) of an atom,
\( n_j = |r_j\rangle \langle r_j| \), 
\( V_{jk} = C/\|r_j - r_k\|^6 \) describes the van der Waals interactions
between atoms, and the detuning is split into a global term 
\( \Delta_{\mathrm{g}}(t) \)
and a site-dependent term \( \Delta_{\mathrm{l}}(t) \), with site 
modulation \( \alpha_j \in [0, 1] \).

By initializing the system in a specific state and allowing 
it to evolve under this Hamiltonian, the resulting quantum 
state encodes information about the input data. Measurements 
of observables on this state yield outputs that can be used 
for tasks such as classification or prediction, with only the 
final readout layer requiring training.


\subsection{Dimensionality Reduction for Image Data}
Dimensionality reduction is a crucial preprocessing step in 
machine learning and data analysis, aiming to reduce the 
number of input variables in a dataset while preserving as 
much information as possible. This process enhances 
computational efficiency, mitigates the curse of 
dimensionality, and facilitates data visualization


\subsubsection{Principal Component Analysis}
Principal Component Analysis (PCA)~\cite{shlensTutorialPrincipalComponent2014} is a linear dimensionality 
reduction technique that transforms a set of correlated 
variables into a set of uncorrelated variables called 
principal components. The goal is to capture the maximum 
variance in the data with the fewest number of components.

PCA is effective for datasets where the principal components 
align with the directions of maximum variance, but it may not 
capture complex, nonlinear relationships in the data~\cite{jolliffePrincipalComponentAnalysis2016}.

\subsubsection{Autoencoder Architectures}
Autoencoders are a class of artificial neural networks 
designed to learn efficient codings of input data in an 
unsupervised manner. They consist of two main parts: an 
encoder that compresses the input into a latent-space 
representation, and a decoder that reconstructs the input 
from this representation.

Given an input \( x \in \mathbb{R}^d \), the encoder maps 
\( x \) to a latent representation \( z \in \mathbb{R}^k \) 
(where \( k < d \)):

\begin{equation}
    z = f_\theta(x)
\end{equation}

The decoder then reconstructs the input:
\begin{equation}
    \hat{x} = g_\phi(z)
\end{equation}
The network is trained to minimize the reconstruction loss:
\begin{equation}
    \mathcal{L}(x, \hat{x}) = \| x - \hat{x} \|^2
\end{equation}

This typical architecture of an autoencoder is illustrated in 
`Fig.-\ref{fig:autoencoder_architecture}'.

\input{images/diagrams/autoencoder_architecture.tex}


Autoencoders can capture complex, nonlinear 
relationships in the data, making them suitable for tasks 
like image compression, denoising, and anomaly 
detection~\cite{hintonReducingDimensionalityData2006, estevaGuideDeepLearning2019}.

\subsubsection{Quantum-Guided Autoencoding}
Quantum-Guided Autoencoders integrate quantum 
computing principles into the autoencoder framework 
to leverage quantum advantages in processing and 
representing data. These models aim to perform 
dimensionality reduction and classification within a 
single architecture, enhancing performance on complex 
datasets.

In the Quantum-Guided Autoencoder (QGAE) model, a 
classical encoder first reduced the dimensionality
of the input data. The compressed data is then processed 
by a parametrized quantum circuit, which acts as the decoder
and classifier. The quantum circuit transforms the 
input state \( |\psi_{\text{in}}\rangle \) into an
output state \( |\psi_{\text{out}}\rangle \) using a unitary operation
\begin{equation}
    |\psi_{\text{out}}\rangle = U(\theta) |\psi_{\text{in}}\rangle
\end{equation}
Measurements on \( |\psi_{\text{out}}\rangle \) yield the final
classification result. The parameters \( \theta \) are
optimized to minimize a loss function that combines
reconstruction error and classification accuracy.


This approach has demonstrated improved performance 
over traditional methods in tasks such as identifying 
the Higgs boson in particle collision data, 
showcasing the potential of quantum-guided models in 
handling high-dimensional, complex datasets~\cite{belisGuidedQuantumCompression2024}.

%============================================
% METHODOLOGY
%============================================
\section{Methodology}
\subsection{Proposed System Architecture}
Our proposed pipeline is a Quantum-Guided Autoencoder with a 
Reservoir Surrogate (QGARS). It is a hybrid architecture that 
synergizes classical autoencoding techniques with quantum reservoir processing to achieve efficient learning of features. These are expected to better match with the reservoir computing layer, whose embeddings are used to perform a classification task.

The system is designed to overcome the inherent non-differentiability of quantum operations, which traditionally impede gradient-based training methods.
The overall architecture is illustrated in `Fig.-\ref{fig:qgars_pipeline}'.

In a higher level, the \textbf{architectural components} are the following:
\begin{itemize}
    \item \textbf{Classical Autoencoder:}
        \begin{itemize}
            \item \textbf{Encoder:}
            The encoder network \(\varepsilon_\omega(\mathbf{x})\)
            transforms high-dimensional input data \(\mathbf{x} \in \mathbb{R}^{n}\) into 
            a lower-dimensional latent representation \(\mathbf{z} \in \mathbb{R}^m\), 
            where \(m < n\).
            \item \textbf{Decoder:}
            The decoder network \(\mathcal{D}_\rho(\mathbf{z})\)
            reconstructs the input data from the latent representation, producing
            \(\mathbf{\hat{x}} \in \mathbb{R}^{n}\).
        \end{itemize}

    \item \textbf{Quantum Reservoir Layer:}
        \begin{itemize}
            \item \textbf{Parameter Mapping:}
            \(g(\mathbf{z})\) maps the latent representation \(\mathbf{z}\) to the local detuning frequencies
            \(\Delta_{\mathrm{i}}\) of the Rydberg Atoms.
            \item \textbf{Quantum Evolution:}
            Evolves the system under a Hamiltonian \(H(\mathbf{\Delta)}\), resulting in a quantum 
            state \(|\psi(\mathbf{\Delta})\rangle\).
            \item \textbf{Measurement:}
            Performs measurements on the quantum state to obtain embeddings \(\mathbf{q} \in \mathbb{R}^k\), which serve as training targets for the Surrogate Model or as 
            inputs for the classification task later on.
        \end{itemize}

    \item \textbf{Surrogate Model:}
    A differentiable classical neural network \( f_{\text{sur}}(\mathbf{z}; \bm{\theta}_{\text{sur}}) \),
    where \(\bm{\theta}_{\text{sur}}\) are parameters tuned to approximate the 
    mapping from \(\mathbf{z}\) to \(\mathbf{q}\), effectively emulating
    the quantum layer's behaviour.

    \item \textbf{Linear Classifier:}
    A classical neural network that maps the quantum embeddings \(\mathbf{q}\) to 
    predicted class labels \(\hat{y}\).

\end{itemize}
The following sections will give a more detailed explanation of each 
component.

\begin{figure*}[!t]
    \centering
    \resizebox{0.9\textwidth}{!}{
        \includegraphics{images/diagrams/qgars_pipeline_cropped.pdf}
    }
    \label{fig:qgars_pipeline}
    \caption{
    Overview of the QGARS pipeline. (a) Classical autoencoder: $\varepsilon_{\omega}(\mathbf{x})$ is the encoder network that maps input $\mathbf{x}$ to latent code $\mathbf{z}$, and $\mathcal{D}_{\rho}(\mathbf{z})$ is the decoder network reconstructing $\hat{\mathbf{x}}$. (b) Rydberg atom quantum reservoir: the latent code $\mathbf{z}$ is mapped to detuning parameters $\Delta_i$, the system evolves under the Hamiltonian in Eq.~\eqref{eq:rydberg_hamiltonian}, and measurements of observables $\langle Z_i\rangle,\langle Z_iZ_j\rangle,\langle Z_iZ_jZ_k\rangle$ yield quantum embeddings. (c) Surrogate model: a feedforward neural network $f_{\mathrm{sur}}(\mathbf{z};\theta_{\mathrm{sur}})$ trained every $N$ epochs to approximate the quantum embeddings. (d) Linear classifier: maps surrogate outputs $\mathbf{q}'$ to class predictions, closing the loop with total loss $\mathcal{L}=(1-\lambda)\mathcal{L}_R+\lambda\mathcal{L}_C$.
    }
\end{figure*}

\subsection{Quantum Guided Autoencoder}
The Quantum Guided Autoencoder (QGA) combines classical 
preprocessing with quantum processing to leverage 
the strengths of both paradigms.

Ideally, the latent space representation created by the encoder \(\varepsilon_\omega(x)\)
would be passed through two sections of the model:
\begin{itemize}
    \item \textbf{The Decoder Network \(\mathcal{D}_\rho(z)\),}
    which tries to reconstruct the original input data.
    \item \textbf{The Quantum Reservoir,}
    which extracts highly dimensional quantum embeddings and
    passes them to a linear classifier to perform a classification
    task and map the embeddings to the image labels.
\end{itemize}
However, some problems emerge when using a real quantum reservoir directly during the autoencoder training stage, which will be discussed in a later section.

\subsubsection{Loss Function Design}
The QGA is trained to minimize a composite loss 
function that balances reconstruction fidelity and 
classification accuracy:

\begin{equation}
    \mathcal{L} = (1-\lambda) \cdot \mathcal{L}_R + \lambda \cdot \mathcal{L}_{C},
    \label{eq:composed_loss_function}
\end{equation}
where:
\begin{itemize}
    \item \( \mathcal{L}_R = \| \mathbf{x} - \hat{\mathbf{x}} \|^2 \) is the reconstruction loss, measuring the mean squared error between the input \( \mathbf{x} \) and its reconstruction \( \mathbf{\hat{x}} \).
    \item \( \mathcal{L}_C = -\sum_i y_i \log(\hat{y}_i) \) is the classification loss, computed as the cross-entropy between the true labels \( y_i \) and the predicted probabilities \( \hat{y}_i \), which are obtained through a linear mapping of the 
    approximated embeddings \(\mathbf{q}'\).
    \item \( 0 < \lambda < 1 \) is a hyperparameter that controls the trade-off between reconstruction and classification objectives.
\end{itemize}    
\subsection{The Gradient Barrier Problem}
Hybrid quantum-classical models, such as the Quantum Guided Autoencoder 
(QGAE), aim to leverage quantum computational advantages within 
classical machine learning frameworks. However, a significant g
challenge arises due to the non-differentiable nature of quantum 
operations, commonly referred to as the `gradient barrier'.

In classical neural networks, training relies on backpropagation, 
which requires the computation of gradients through all components 
of the network. In hybrid models, the inclusion of quantum 
layers introduces operations that are inherently non-differentiable:

\begin{itemize}
    \item \textbf{Quantum State Preparation}: 
    Encoding classical data into 
    quantum states often involves operations 
    that are not smoothly differentiable with 
    respect to the input data.
    
    \item \textbf{Quantum Measurements}:
    Observing quantum states collapses them, 
    introducing stochasticity and breaking the 
    deterministic gradient flow required 
    for backpropagation.
\end{itemize}

These aspects hinder the direct application of 
gradient-based optimization methods across the 
quantum-classical boundary, obstructing end-to-end 
training of hybrid models. The surrogate model 
solution, proposed in the next section, addresses 
this problem.


\subsection{Surrogate Modeling for Quantum Layers}
To circumvent the gradient barrier, surrogate models 
have been proposed. These are classical, differentiable 
models trained to approximate the input-output behavior 
of quantum circuits. By replacing the non-differentiable 
quantum components with their surrogate counterparts during 
training, gradients can be propagated through the entire network, 
enabling end-to-end optimization. 

Recent studies have demonstrated the efficacy of 
surrogate models in mitigating the effects of barren plateaus 
and facilitating the training of hybrid quantum-classical models. 
These approaches allow for the practical implementation of 
quantum-enhanced machine learning models by leveraging classical 
optimization techniques while still capturing quantum computational 
advantages~\cite{xieQuantumSurrogateDrivenImage2025b}


\subsubsection{Surrogate Model Architecture}
In our pipeline, the surrogate model \( f_{\text{sur}}(\mathbf{z}; \theta_{\text{sur}}) \) is implemented as a feedforward neural network designed to learn the mapping from the autoencoder's latent space to the quantum embeddings produced by the quantum reservoir. 
\begin{equation}
    f_{\text{sur}}(\mathbf{z}; \theta_{\text{sur}}) = \mathbf{q}' \approx \mathbf{q} = \mathrm{Measure}(|\psi(g(\mathbf{z})\rangle)
\end{equation}
Its architecture comprises multiple fully connected 
layers with nonlinear activation functions, facilitating 
the approximation of the complex, non-linear 
transformations inherent in quantum dynamics.

This approach draws inspiration from recent studies 
demonstrating the effectiveness of surrogate models 
in approximating quantum circuit behaviors, thereby 
facilitating efficient training of hybrid 
models.~\cite{xieQuantumSurrogateDrivenImage2025b,schreiberClassicalSurrogatesQuantum2023}

\subsubsection{Training Procedure}
\begin{itemize}
    \item Data Input: 
    Feed a batch of latent data \( \mathbf{z} \) from the autoencoder 
    into the real quantum reservoir, recording the resulting quantum 
    embeddings.

    \item Surrogate Training:
    Train the surrogate model  by minimizing the loss function:
    \begin{equation}
        \mathcal{L}_{\text{sur}} = \| q' - q \|^2,
    \end{equation}

    
    \item Regular Updates:
    Periodically update the surrogate model during training
    to ensure it accurately reflects the quantum reservoir's
    behavior. This can be done after a fixed number of epochs
    or when the surrogate's performance on a validation set
    reaches a certain threshold.
\end{itemize}

\subsubsection{Gradient Flow Through Surrogate Models}
By integrating the surrogate model into the training pipeline, 
we establish a continuous computational graph from the 
output layer back to the input layer, enabling the use of 
gradient-based optimization techniques. The surrogate model 
serves as a differentiable proxy for the quantum reservoir, 
allowing the classification loss to influence the 
autoencoder's parameters effectively.

As a result, we not only achieve end-to-end training of the
hybrid model but also makes it computationally efficient,
as we reduce the need for frequent quantum measurements.

\subsection{Rydberg Hamiltonian and Quantum Dynamics}
Although it can seem that the surrogate model is enough to cover the job of the quantum reservoir, that is far from the truth. while a fixed classical reservoir can nonlinearly mix inputs, its capacity is polynomially bounded. In contrast, our neutral-atom reservoir leverages many-body quantum dynamics to encode data in a space whose dimension grows exponentially with atom number, unlocking patterns that classical reservoirs would need unreasonably complex architectures to approximate. This means that the quantum reservoir is a crucial part of our pipeline and that the surrogate is just an approximation to aid in the autoencoder training process.

The quantum reservoir in our architecture is implemented
using a neutral-atom system, specifically utilizing
Rydberg atoms to create a programmable quantum reservoir.
The dynamics of the system are governed by the Rydberg Hamiltonian,
which describes the interactions between atoms in Rydberg states~\ref{eq:rydberg_hamiltonian}.


\subsubsection{Data Encoding Schemes}
The input data is encoded into the quantum system by mapping
the latent representations from the autoencoder to the
detuning parameters of the Rydberg Hamiltonian.
This encoding process involves adjusting the detuning
parameters \( \Delta_{\mathrm{l}}(t) \) for each atom in the chain,
allowing the system to represent the input data in a way that
is compatible with the quantum dynamics of the reservoir.

\subsubsection{Quantum Readout Methods}
The quantum reservoir's output is obtained by probing, 
in successive time steps, the state of the atoms 
after the systems evolution up to a certain time \( t \).
The readout is performed by measuring specific observables,
such as $\langle Z_i \rangle$, $\langle Z_i \, Z_j\rangle$, and $\langle Z_i \, Z_j \, Z_k \rangle$, which measure respectively one-, two- and three-qubit correlations.
% \begin{itemize}
%     \item \textbf{Single-atom Measurements}: 
%     \( \langle Z_i \rangle \) for each atom \( i \), where \( Z_i \) is the Pauli-Z operator.
%     \item \textbf{Two-atom Correlations}: 
%     \( \langle Z_i Z_j \rangle \) for pairs of atoms \( (i, j) \), capturing correlations between their states.
%     \item \textbf{Three-atom Correlations}:
%     \( \langle Z_i Z_j Z_k \rangle \) for triplets of atoms \( (i, j, k) \), providing higher-order correlations.
% \end{itemize}


%============================================
% EXPERIMENTAL SETUP
%============================================
\section{Experimental Setup}
\subsection{Datasets}
We evaluate our proposed architecture on three primary datasets:
\begin{enumerate}
    \item \textbf{Synthetic Polyp Dataset:} 
    A synthetic dataset of polyp images generated to simulate 
    realistic medical imaging scenarios.
    
    \item \textbf{CVC-ClinicDB Dataset:} 
    Real image patches extracted from the CVC-ClinicDB 
    dataset, a well-known benchmark for polyp detection. 
    This dataset provides a diverse set of polyp images 
    with varying characteristics, allowing for robust 
    evaluation of classification performance.
    
    \item \textbf{MNIST Dataset:} 
    A reduced version of the MNIST dataset, 
    containing only the digits 0 and 1, suitable 
    for binary classification tasks. This dataset 
    serves as a controlled environment to assess 
    the model's performance on simpler classification tasks.
\end{enumerate}

\subsection{Feature Reduction Methods}
We implemented and compared three distinct feature reduction approaches:
\subsubsection{Principal Component Analysis (PCA)}
PCA was employed as a baseline linear 
dimensionality reduction technique. 
We extracted the top-k principal components 
(typically 4-12) from the flattened image data, 
which were then used as inputs to both classical and 
quantum-enhanced classifiers.

\subsubsection{Autoencoder}
We implemented a neural network-based autoencoder with the following architecture:

\begin{itemize}
    \item An encoder network that compresses
    the input to a lower-dimensional latent space.
    
    \item A decoder network that reconstructs 
    the original input from the latent representation

    \item Batch normalization and dropout layers 
    to improve training stability and prevent overfitting.
\end{itemize}

The autoencoder was trained to minimize reconstruction 
loss using the Adam optimizer with learning rates 
between $0.001$--$0.01$ and weight decay for regularization.

\subsubsection{Quantum-Guided Autoencoder}
We introduced a quantum-guided autoencoder 
that incorporates feedback from the quantum 
reservoir during training. This approach combines:

\begin{itemize}
    \item A standard autoencoder architecture
    \item A linear classifier that maps quantum embeddings to class labels.
    \item A classification loss computed on quantum embeddings
    \item A weighting parameter $\lambda$ to control the trade-off between 
    reconstruction and classification objectives~\ref{eq:composed_loss_function}.
    \item A differentiable surrogate model that approximates the quantum reservoir's behavior,
    enabling end-to-end backpropagation through the entire pipeline. The frequency 
    of quantum updates is controlled by a hyperparameter, allowing for
    flexible integration of quantum dynamics into the training process.
\end{itemize}


\subsection{Quantum Reservoir Computing Layer}
Our quantum reservoir computing implementation uses 
a Rydberg atom simulator with the following parameterizable
components:

\begin{itemize}
    \item \textbf{Atom Chain Length:} 
    The number of atoms in the chain, which can be adjusted 
    to control the reservoir's capacity and complexity.
    
    \item \textbf{Rabi Frequency:} 
    The global Rabi drive amplitude, which influences the 
    strength of interactions between atoms.
    
    \item \textbf{Detuning Parameters:} 
    Site-dependent detuning parameters that encode the input data.
    
    \item \textbf{Measurement Strategy:} 
    The specific observables measured to obtain quantum embeddings.

    \item \textbf{Encoding Scale:}
    The scale of the input data encoding, which can be adjusted
    to optimize the quantum reservoir's response to the input data.

    \item \textbf{Time steps:}
    The number of time steps for which the quantum reservoir evolves,
    allowing for the capture of temporal dynamics in the data.
\end{itemize}

\subsection{QRC Embeddings Linear Classifier}
After the traning process of the QGARS model,
the following quantum embeddings are passed to a linear classifier.
The classifier is a simple feedforward neural network with the following architecture:
\begin{itemize}
    \item A single hidden layer with a ReLU activation function.
    \item Cross-entropy loss for training, optimized using the Adam optimizer.
\end{itemize}

\subsection{Comparison Methods}
We benchmarked our model against some
classical methods, including:
\begin{itemize}
    \item \textbf{PCA/Autoencoder + Linear Classifier:} 
    A linear classifier trained on the PCA-reduced features or the latent representations
    from the autoencoder.
    
    \item \textbf{PCA/Autoencoder + Neural Network:}
    A fully connected neural network trained on the features from the 
    same reduction methods as the linear classifier. 
\end{itemize}

We also benchmarked our QGARS encoding strategy
against the performance of the Linear Mapping of
QRC embeddings extracted from the latent representation 
of data using other feature reduction methods, such as
PCA and the Classical Autoencoder.

\subsection{Performance Metrics}
We evaluated the performance of our model using the following metrics:
Classification Accuracy, F1 Score, Confusion Matrix.

% \begin{itemize}
%     \item \textbf{Classification Accuracy} 
%     %The percentage of correctly classified instances in the test set.
    
%     \item \textbf{F1 Score} 
%     %The harmonic mean of precision and recall, providing a balanced measure of %classification performance.
    
%     \item \textbf{Confusion Matrix} 
%    % A matrix summarizing the classification results, showing true positives, %false positives, true negatives, and false negatives.
    
% \end{itemize}


\subsection{Parameter Sweep Strategy}
To optimize the performance of our quantum-guided autoencoder, 
we conducted a systematic parameter sweep over the following hyperparameters:
\begin{itemize}
    \item \textbf{Guided Lambda Parameter (\( \lambda \))}: 
    To analyse the effect that the trade-off between reconstruction and classification objectives in the loss function~\ref{eq:composed_loss_function} affects
    the QRC
    
    \item \textbf{Quantum Update Frequency:} 
    The frequency at which the quantum reservoir is updated during training. We experimented with update frequencies of 1, 5, and 10 epochs.

    \item \textbf{Quantum Parameters:} 
    Rabi frequency, and detuning parameters. We varied these parameters to assess their impact on classification performance.

    \item \textbf{Number of Features:}
    The number of features is the same as the number of qubits (atoms), 
    as each feature encoded in each atom.
    
\end{itemize}
The results from these experiments will be discussed in the next section.

%============================================
% RESULTS
%============================================
\section{Results and Discussion}

\subsection{Classification Performance Comparison}
We first compare classification accuracy of the Linear Classifier on the QRC embeddings across PCA,
vanilla autoencoder, and our QGARS pipeline on each dataset. 
Table~\ref{tab:qrc_reduction_accuracy} summarizes mean accuracy and standard deviation over 
five runs, using 12 features.

\begin{table}[!htb]
  \caption{Classification accuracy (\%) on test sets}
  \label{tab:qrc_reduction_accuracy}
  \centering
  \begin{tabular}{lccc}
    \hline
    Method & Synthetic Polyp & CVC-ClinicDB & MNIST(0/1) \\
    \hline
    PCA + QRC + Linear      &  00 ± 0.0  &  00 ± 0.0  &  00 ± 0.0 \\
    AE + QRC + Linear       &  00 ± 0.0  &  00 ± 0.0  &  00 ± 0.0 \\
    QGARS + QRC + Linear    &  \textbf{00 ± 0.0} &  \textbf{00 ± 0.0} &  \textbf{00 ± 0.0} \\
    \hline
  \end{tabular}
\end{table}


\subsection{Dimensionality Reduction Comparison}

To evaluate how well the quantum reservoir embeddings separate classes, we project the high-dimensional embeddings down to $2$ dimensions via t-distributed Stochastic Neighbour Embedding (t-SNE). Figure~\ref{fig:embeddings_tsne} shows side-by-side plots for PCA → QRC, AE → QRC, and QGARS → QRC pipelines. The QGARS embeddings exhibit the tightest and most distinct clusters, indicating superior discriminative structure.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.32\textwidth]{pca_qrc_tsne.pdf}
  \includegraphics[width=0.32\textwidth]{ae_qrc_tsne.pdf}
  \includegraphics[width=0.32\textwidth]{qgars_qrc_tsne.pdf}
  \caption{3-dimensional t-SNE projections of the 4768-dimensional QRC embeddings for (left) PCA + QRC, (center) AE + QRC, (right) QGARS + QRC}
  \label{fig:embeddings_tsne}
\end{figure*}

Quantitatively, we compute the Silhouette Score on the original 4 700-D embeddings (Table~\ref{tab:silhouette}). The QGARS pipeline achieves the highest score, confirming better inter-class separation:

\begin{table}[ht]
  \caption{Silhouette Scores for QRC embeddings}
  \label{tab:silhouette}
  \centering
  \begin{tabular}{lcc}
    \hline
    Pipeline         & Silhouette Score \\
    \hline
    PCA + QRC        & 0.0 \\
    AE + QRC         & 0.0 \\
    QGARS + QRC      & \textbf{0.0} \\
    \hline
  \end{tabular}
\end{table}


\subsection{Ablation Studies}

\subsubsection{Impact of Guided Lambda Parameter}
We vary the guidance weighting $\lambda$ in Eq.~\ref{eq:composed_loss_function} to assess its influence on performance. Figure~\ref{fig:lambda_sweep} plots test accuracy vs.\ $\lambda$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{paper/images/results/generated_polyp_dataset/guided_lambda_log_mu_effect.pdf}
  \caption{Accuracy as a function of the reconstruction/classification trade-off parameter $\lambda$.}
  \label{fig:lambda_sweep}
\end{figure}

\noindent
We find optimal performance around $\lambda=XXX$; too little guidance ($\lambda<XXX$) yields under-trained classifiers, while too much ($\lambda>XXX$) degrades reconstruction quality.


\subsubsection{Effect of Quantum Update Frequency}

\subsubsection{Influence of Quantum Parameters}

\subsection{Surrogate Model Fidelity Analysis}
We assess how well the surrogate approximates the quantum reservoir by 
computing the MSE between true quantum embeddings and surrogate outputs over the test set (Figure~\ref{fig:surrogate_fidelity}). Low MSE indicates high fidelity.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{surrogate_mse.pdf}
  \caption{Surrogate vs.\ quantum embedding MSE over training epochs.}
  \label{fig:surrogate_fidelity}
\end{figure}


\subsection{Comparison with Classical Methods}
To benchmark the benefit of our QGARS pipeline, 
we compare against purely classical feature-reduction 
approaches followed by standard classifiers. 
Table~\ref{tab:classical_comp} reports test accuracy on each dataset 
for the different methods.

\begin{table}[ht]
  \caption{Test set classification accuracy (\%) for classical baselines}
  \label{tab:classical_comp}
  \centering
  \begin{tabular}{lccc}
    \hline
    Method            & Synthetic Polyp & CVC-ClinicDB & MNIST (0/1) \\
    \hline
    PCA + Linear      &  0.0 ± 0.0     &  0.0 ± 0.0   &  0.0 ± 0.0    \\
    AE + Linear       &  0.0 ± 0.0     &  0.0 ± 0.0   &  0.0 ± 0.0    \\
    PCA + NN          &  0.0 ± 0.0     &  0.0 ± 0.0   &  0.0 ± 0.0    \\
    AE + NN           &  0.0 ± 0.0     &  0.0 ± 0.0   &  0.0 ± 0.0    \\
    \hline
  \end{tabular}
\end{table}

\noindent
In contrast, the QGARS pipeline achieves 0.0\%, 0.0\%, and 0.0\% on the same datasets (see Table~\ref{tab:qrc_reduction_accuracy}).

\subsubsection{Training Loss Dynamics}

Figure~\ref{fig:loss_curves} shows the training loss curves for the classical baselines and QGARS on the Synthetic Polyp dataset. We plot the reconstruction loss for PCA/AE methods combined with classification loss for the AE+NN and QGARS pipelines.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{loss_comparison.pdf}
  \caption{Normalized training loss vs.\ epoch for PCA+Linear, AE+Linear, AE+NN, and QGARS on the Synthetic Polyp dataset.}
  \label{fig:loss_curves}
\end{figure}

\noindent
Although the loss function of the QGARS method shows
signs of instability, is not only converges faster but also reaches a 
lower combined loss

%============================================
% LIMITATIONS AND FUTURE WORK
%============================================
\section{Limitations and Future Work}
\subsection{Current Limitations}
While QGARS demonstrates strong performance, several limitations remain:
\begin{itemize}
    \item \textbf{Surrogate Fidelity:}
    Imperfect surrogate approximation can introduce bias in the gradients and limit end‐to‐end 
    performance, especially if the latent distribution shifts significantly during training.
    \item \textbf{Classical overhead:}  
    Frequent surrogate retraining and batch quantum simulations introduce 
    nontrivial classical compute overhead, potentially offsetting some of the 
    quantum advantage.
\end{itemize}

\subsection{Potential Extensions}
Several avenues can further enhance QGARS:
\begin{itemize}
    \item \textbf{Different Hamiltonians:}
    Explore alternative interaction Hamiltonians (e.g., XY, Heisenberg) to better tailor reservoir dynamics, as experimented by A. D. Lorenzis et al~\cite{lorenzisHarnessingQuantumExtreme2025}.

    \item \textbf{Convolutional Autoencoders:}
    Replace the fully connected autoencoder with a convolutional AE to learn spatially localized features, making the model input‐size agnostic and enabling richer local structure for the reservoir.
    
    \item \textbf{Reservoir encoding schemes:}
    Investigate alternate encodings—such as position encoding via atom placement, global versus local pulse shaping, or multi‐tone driving—to diversify the reservoir’s nonlinear response.
    
    \item \textbf{\(\lambda\)-Scheduling \& Curriculum Learning:}  
    Implement a curriculum that gradually increases the guidance weight \(\lambda\) from 0 to 1, allowing the autoencoder to first stabilize reconstruction before introducing quantum guidance. Hybrid ramping schedules may accelerate convergence.
    
    \item \textbf{Noise‐Aware Training:}  
    Incorporate realistic noise models (decoherence, measurement error) into surrogate training so that the surrogate learns hardware‐like quantum embeddings, improving robustness upon deployment in NISQ devices.

\end{itemize}

\subsection{Hardware Implementation Considerations}
Bringing QGARS to physical hardware entails additional challenges:
\begin{itemize}
    \item \textbf{Calibration and drift:}  
        Rydberg atom platforms require careful calibration of laser frequencies and intensities; drift over time may degrade reservoir fidelity.
    \item \textbf{Shot noise and sampling cost:}  
        Finite‐shot measurements introduce statistical noise; optimizing shot budgets versus surrogate fidelity is critical.
    \item \textbf{Scalability:}  
        Extending to hundreds of atoms demands sophisticated trap control and cross‐talk mitigation; resource scheduling on shared QPUs may limit throughput.
    \item \textbf{Integration with classical pipelines:}  
        Efficient data transfer between classical and quantum subsystems (e.g.\ via FPGA or GPU–QPU interconnects) and asynchronous surrogate updates will be essential for low‐latency operation.
\end{itemize}

%============================================
% CONCLUSION
%============================================
\section{Conclusion}

We have presented QGARS, a hybrid quantum-classical architecture that integrates a classical autoencoder with a neutral-atom quantum reservoir and a differentiable surrogate model to enable end‐to‐end training. By jointly optimizing reconstruction and classification losses, QGARS learns compact and discriminative features tailored for quantum reservoir computing. Our experiments on synthetic polyp, CVC-ClinicDB, and binary MNIST datasets demonstrate that QGARS outperforms classical PCA‐ and autoencoder‐based baselines. Ablation studies highlight the importance of the guidance weight \(\lambda\), surrogate update frequency, and reservoir parameters. While challenges remain—particularly regarding high‐dimensional embeddings, surrogate fidelity, and classical overhead, our proposed extensions and hardware considerations chart a clear path toward robust, scalable quantum‐enhanced medical imaging pipelines in the NISQ era.

% ADICIONAR COMENTÁRIO SOBRE COMO É MELHOR ESTUDAR MAIS O PARÂMETRO DE LAMBDA


%============================================
% REFERENCES
%============================================
\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}